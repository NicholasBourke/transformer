Transformer architecture as described in _Attention Is All You Need_ (Vaswani et.al, 2017). Coded using PyTorch, however with only the most atomic elements of the model's structure handled using PyTorch modules. The only modules used are Linear, Embedding, Parameter, Dropout, ReLU, and ModuleList.

The primary intention was to gain deeper understanding of the Transformer network architecture by replicating it from the source (ie. the research paper itself). No models were trained using this exact architecture, however it has served as the basis of other models based on the transformer (eg. GPT, ViT) which have been trained successfully. The structure was left as general as possible, with no particular application in mind. Appropriate modifications would be expected for use on any given application.

The architecture code is contained in model.py, while main.py contains a simple example of initializing an instance with the model configuration dataclass, and some dummy data to show forward propagation functionality.